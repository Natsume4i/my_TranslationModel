import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.nn.utils.rnn import pad_sequence
# åŠ è½½è‹±æ—¥ç¿»è¯‘æ•°æ®é›†ï¼ˆå¯ç”¨è¿œç¨‹ä»£ç ä¿¡ä»»ï¼‰
dataset = load_dataset(
    "iwslt2017",
    "iwslt2017-en-ja",
    trust_remote_code=True
)

train_data = dataset["train"]
val_data = dataset["validation"]
test_data = dataset["test"]

src_lan = 'en'    # æºè¯­è¨€ï¼šè‹±è¯­
tgt_lan = 'ja'    # ç›®æ ‡è¯­è¨€ï¼šæ—¥è¯­

# ä½¿ç”¨spacyè‹±è¯­åˆ†è¯å™¨ï¼Œæ—¥æœ¬è¯­ç”¨basicç©ºæ ¼åˆ†
token_transform = {
    src_lan: get_tokenizer('spacy', language='en_core_web_sm'),
    tgt_lan: lambda x: list(x)  # ç®€å•å¤„ç†æ—¥è¯­ï¼ˆæ¯ä¸ªå­—åˆ†å¼€ï¼‰
}

def yield_tokens(data_iter, language):
    for item in data_iter:
        yield token_transform[language](item["translation"][language])

#å°†è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªå¥å­åˆ†æˆæ¯ä¸€ä¸ªè¯ï¼Œå†å°†æ¯ä¸€ä¸ªè¯éƒ½å¯¹åº”ä¸€ä¸ªæ•°å­—
# æ„å»ºæºè¯­è¨€ï¼ˆè‹±è¯­ï¼‰è¯è¡¨
#æ³¨æ„çœ‹ï¼Œæ˜¯ä»è®­ç»ƒæ•°æ®é›†æ¥çš„
SRC_VOCAB = build_vocab_from_iterator(yield_tokens(train_data, src_lan), specials=["<unk>", "<pad>", "<bos>", "<eos>"])
SRC_VOCAB.set_default_index(SRC_VOCAB["<unk>"])

# æ„å»ºç›®æ ‡è¯­è¨€ï¼ˆæ—¥è¯­ï¼‰è¯è¡¨
TGT_VOCAB = build_vocab_from_iterator(yield_tokens(train_data, tgt_lan), specials=["<unk>", "<pad>", "<bos>", "<eos>"])
TGT_VOCAB.set_default_index(TGT_VOCAB["<unk>"])

def tensor_transform(token_ids, vocab):
    return torch.cat((
        torch.tensor([vocab['<bos>']],dtype=torch.long),
        torch.tensor(token_ids, dtype=torch.long),
        torch.tensor([vocab['<eos>']],dtype=torch.long)
    ))

#å°†å¥å­è½¬åŒ–ä¸ºä¸€ä¸²æ•°å­—
text_transform = {
    src_lan : lambda x : [SRC_VOCAB[token] for token in token_transform[src_lan](x)],
    tgt_lan : lambda x : [TGT_VOCAB[token] for token in token_transform[tgt_lan](x)]
}

def collate_fn(batch):
    src_batch, tgt_batch = [], []
    for item in batch:
        src_text = item["translation"][src_lan]
        tgt_text = item["translation"][tgt_lan]
        # ä¸€ä¸²æ•°å­—å†è½¬ä¸ºtensor
        src_tokens = tensor_transform(text_transform[src_lan](src_text), SRC_VOCAB)
        tgt_tokens = tensor_transform(text_transform[tgt_lan](tgt_text), TGT_VOCAB)

        src_batch.append(src_tokens)
        tgt_batch.append(tgt_tokens)

    src_batch = pad_sequence(src_batch, padding_value=SRC_VOCAB["<pad>"])
    tgt_batch = pad_sequence(tgt_batch, padding_value=TGT_VOCAB["<pad>"])
    return src_batch, tgt_batch

BATCH_SIZE = 32

train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)
test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)



import torch.nn as nn

class Seq2SeqTransformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers,
                 emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,
                                          num_encoder_layers=num_encoder_layers,
                                          num_decoder_layers=num_decoder_layers,
                                          dim_feedforward=dim_feedforward,
                                          dropout=dropout)

        self.generator = nn.Linear(emb_size, tgt_vocab_size)
        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):
        #å°†æ•°å­—åºåˆ—tensoråŠ ä¸Šä½ç½®ç¼–ç 
        #encoderçš„ä½“ç°
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        return self.generator(outs)
    #forwardè¿”å›çš„æ˜¯æœ€ç»ˆç»“æœ

    def encode(self, src, src_key_padding_mask=None):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        return self.transformer.encoder(src_emb, mask=None, src_key_padding_mask=src_key_padding_mask)

    def decode(self, tgt, memory, tgt_mask):
        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory,
                                        tgt_mask)

import math

class PositionalEncoding(nn.Module):
    def __init__(self, emb_size, dropout, maxlen=5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

def generate_square_subsequent_mask(sz, device):
    """
    è¿”å› [sz, sz] ä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äº Decoder é˜²æ­¢çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ã€‚
    å¯¹è§’çº¿åŠä»¥ä¸‹ä¸º 0ï¼Œå¯¹è§’çº¿ä»¥ä¸Šä¸º -infã€‚
    """
    mask = torch.triu(torch.ones((sz, sz), device=device), diagonal=1)
    mask = mask.masked_fill(mask == 1, float('-inf'))
    return mask

def create_mask(src, tgt_input, src_pad_idx, tgt_pad_idx):
    """
    æ ¹æ® src å’Œ tgt_inputï¼Œåˆ›å»º Transformer æ‰€éœ€çš„æ‰€æœ‰ maskã€‚
    """
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt_input.shape[0]
    device = src.device

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)

    src_padding_mask = (src == src_pad_idx).transpose(0, 1)  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_input == tgt_pad_idx).transpose(0, 1)  # [batch_size, tgt_seq_len]

    return None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask

import os
SRC_VOCAB_SIZE = len(SRC_VOCAB)
TGT_VOCAB_SIZE = len(TGT_VOCAB)
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 32
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD,
                           SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
model = model.to(device)

PAD_IDX = TGT_VOCAB['<pad>']
criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

checkpoint_path = "checkpoint.pth"
start_epoch = 1

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    print(f"âœ… åŠ è½½ checkpoint æˆåŠŸï¼Œæ¢å¤è®­ç»ƒä»ç¬¬ {start_epoch} è½®å¼€å§‹")
else:
    print("ğŸ†• æœªå‘ç° checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")


def train_epoch(model, optimizer):
    model.train()#å¼€å¯è®­ç»ƒæ¨¡å¼
    total_loss = 0
    for src, tgt in train_iter:
        #srcå’Œtgtä¸ºtensor
        src = src.to(device)
        tgt = tgt.to(device)

        tgt_input = tgt[:-1, :]#å»æ‰eosï¼Œè¦ä¼ å…¥decoderçš„ï¼Œ(è™½è¯´æ˜¯æ•´ä¸ªå¥å­ä¼ å…¥ï¼Œä½†æ˜¯ä¼šæœ‰maskåˆ¶çº¦)
        targets = tgt[1:, :].reshape(-1)#å»æ‰bosï¼Œå¹¶å˜æˆä¸€ç»´å‘é‡

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask = create_mask(
            src, tgt_input, SRC_VOCAB['<pad>'], TGT_VOCAB['<pad>']
        )
        #ç”Ÿæˆå„ç§mask
        logits = model(src, tgt_input, src_mask, tgt_mask,
                       src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        # å¾—åˆ°æ¯ä¸ªä½ç½®æ¯ä¸ªå¥å­å¯¹åº”è¯è¡¨çš„æ¦‚ç‡ï¼Œæ¨¡å‹çš„æœ€ç»ˆè¿è¡Œç»“æœ(æœªç»è¿‡softmax)

        optimizer.zero_grad()
        loss = criterion(logits.reshape(-1, logits.shape[-1]), targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_iter)

def evaluate(model):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for src, tgt in val_iter:
            src = src.to(device)
            tgt = tgt.to(device)

            tgt_input = tgt[:-1, :]
            targets = tgt[1:, :].reshape(-1)#å®é™…ä¸Šæœ‰å¤šä¸ªå¥å­(å…·ä½“æ¥è¯´æœ‰batch sizeä¸ª)

            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask = create_mask(
                src, tgt_input, SRC_VOCAB['<pad>'], TGT_VOCAB['<pad>']
            )

            logits = model(src, tgt_input, src_mask, tgt_mask,
                           src_padding_mask, tgt_padding_mask, memory_key_padding_mask)

            loss = criterion(logits.reshape(-1, logits.shape[-1]), targets)
            #å°†ä¸‰ç»´çš„logitsé™ä¸º2ç»´çš„
            total_loss += loss.item()
    return total_loss / len(val_iter)


NUM_EPOCHS = 10

for epoch in range(start_epoch, NUM_EPOCHS + 1):
    train_loss = train_epoch(model, optimizer)
    val_loss = evaluate(model)

    print(f"ğŸ“˜ Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # ä¿å­˜ checkpoint
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }, checkpoint_path)
    print(f"å·²ä¿å­˜ç¬¬ {epoch} è½®çš„ checkpoint")

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# ç¡®ä¿ä¸‹è½½äº† nltk çš„åˆ†è¯å·¥å…·
nltk.download('punkt')

def greedy_decode(model, src, src_key_padding_mask, max_len, start_symbol):
    src = src.to(device)
    memory = model.encode(src, src_key_padding_mask=src_key_padding_mask)

    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)

    for i in range(max_len - 1):
        tgt_mask = generate_square_subsequent_mask(ys.size(0), device).type(torch.bool)
        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        next_word = torch.argmax(prob, dim=1).item()
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == TGT_VOCAB['<eos>']:
            break

    return ys


def tokens_to_sentence(tokens, vocab):
    itos = list(vocab.get_itos())  # è·å–ç´¢å¼•åˆ°è¯çš„åˆ—è¡¨
    words = []
    for token in tokens:
        word = itos[token]
        if word in ['<bos>', '<pad>']:
            continue
        if word == '<eos>':
            break
        words.append(word)
    return words




def compute_bleu_score(model, data_iter, num_samples=100):
    model.eval()
    total_bleu = 0.0
    smooth_fn = SmoothingFunction().method1

    count = 0
    for src, tgt in data_iter:
        src = src.to(device)
        tgt = tgt.to(device)

        for i in range(src.shape[1]):  # batch ç»´åº¦
            src_sent = src[:, i].unsqueeze(1)
            tgt_sent = tgt[1:, i]  # å»æ‰ <bos>

            src_key_padding_mask = (src_sent.squeeze(1) == SRC_VOCAB['<pad>']).unsqueeze(0)
            pred_tokens = greedy_decode(
                model, src_sent, src_key_padding_mask=src_key_padding_mask,
                max_len=50, start_symbol=TGT_VOCAB['<bos>']
            )
            pred_sentence = tokens_to_sentence(pred_tokens.flatten().cpu().numpy(), TGT_VOCAB)
            tgt_sentence = tokens_to_sentence(tgt_sent.cpu().numpy(), TGT_VOCAB)

            bleu = sentence_bleu([tgt_sentence], pred_sentence, smoothing_function=smooth_fn)
            total_bleu += bleu
            count += 1

            if count >= num_samples:
                break
        if count >= num_samples:
            break

    avg_bleu = total_bleu / count
    print(f"Average BLEU score on {count} samples: {avg_bleu:.4f}")
    return avg_bleu

compute_bleu_score(model, val_iter, num_samples=100)
